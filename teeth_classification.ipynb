{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "FEATURE_SIZE = 10\n",
    "FEATURE_CLASS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    img = skimage.io.imread(path)\n",
    "    img = img / 255.0\n",
    "    # print \"Original Image Shape: \", img.shape\n",
    "    # we crop image from center\n",
    "    short_edge = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "    # resize to 224, 224\n",
    "    resized_img = skimage.transform.resize(crop_img, (IMG_SIZE, IMG_SIZE))[None, :, :, :]   # shape [1, 224, 224, 3]\n",
    "#    resized_img = skimage.transform.resize(crop_img, (32, 32))[None, :, :, :]   # shape [1, 32, 32, 3]\n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_dir, part_dir_list):\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    feat_list = []\n",
    "    feat_array = np.linspace(0, 1, FEATURE_CLASS)\n",
    "    \n",
    "    for k in range(len(part_dir_list)):\n",
    "        part_name = part_dir_list[k]\n",
    "        dir = image_dir + part_name\n",
    "        \n",
    "        index = 0\n",
    "        if k==0 or k==3 or k==7 or k==10:   #category 1\n",
    "            index = 0\n",
    "        elif k==2:\n",
    "            index = 2\n",
    "        elif k==5:\n",
    "            index = 3\n",
    "        else:\n",
    "            index = 1\n",
    "        \n",
    "        for file in os.listdir(dir):\n",
    "            if not file.lower().endswith('.jpg'):\n",
    "                continue\n",
    "            try:\n",
    "                resized_img = load_img(os.path.join(dir, file))\n",
    "            except OSError:\n",
    "                continue\n",
    "            image_list.append(resized_img)    # [1, height, width, depth] * n\n",
    "            \n",
    "            tag = np.zeros((1, len(part_dir_list)))\n",
    "            tag[0][k] = 1\n",
    "            label_list.append(tag)\n",
    "            \n",
    "            feature = np.full((1, FEATURE_SIZE), feat_array[index])\n",
    "            feat_list.append(feature)\n",
    "\n",
    "#            if len(imgs[k]) == 400:        # only use 400 imgs to reduce my memory load\n",
    "#                break\n",
    "    \n",
    "    image_data = np.concatenate(image_list, axis=0)\n",
    "    label_data = np.concatenate(label_list, axis=0)\n",
    "    feat_data = np.concatenate(feat_list, axis=0)\n",
    "    \n",
    "    return image_data, label_data, feat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image_dir = 'train_img/'\n",
    "test_image_dir = 'test_img/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part_list = ['out', 'in']\n",
    "part_list = ['in_down_left', 'in_down_right', 'in_down_center', 'in_up_left', 'in_up_right', 'in_up_center',\n",
    "            'out_down_left', 'out_down_right', 'out_down_center', 'out_up_left', 'out_up_right', 'out_up_center']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yu-hsunchen/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Users/yu-hsunchen/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "train_dataset, label_dataset, feature_dataset = load_data(train_image_dir, part_dir_list=part_list)\n",
    "test_dataset, test_label, test_feature = load_data(test_image_dir, part_dir_list=part_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 32, 32, 3)"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dataset\n",
    "label_dataset.shape\n",
    "label_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33333333,  0.33333333,  0.33333333,  0.33333333,  0.33333333,\n",
       "        0.33333333,  0.33333333,  0.33333333,  0.33333333,  0.33333333])"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dataset.shape\n",
    "feature_dataset[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c77ba30b8>"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c743f03c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_dataset[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape=[None, IMG_SIZE, IMG_SIZE, 3])\n",
    "#x = tf.placeholder(tf.float32,shape=[None,32,32,3])\n",
    "y_true = tf.placeholder(tf.float32,shape=[None,len(part_list)])\n",
    "x_feat = tf.placeholder(tf.float32,shape=[None,FEATURE_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convo_1 = convolutional_layer(x,shape=[4,4,3,32])\n",
    "convo_1_pooling = max_pool_2by2(convo_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convo_2 = convolutional_layer(convo_1_pooling,shape=[4,4,32,64])\n",
    "convo_2_pooling = max_pool_2by2(convo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convo_2_flat = tf.reshape(convo_2_pooling, [-1, 8*8*64])  #example\n",
    "size = (int)(IMG_SIZE/4)\n",
    "convo_2_flat = tf.reshape(convo_2_pooling, [-1, size*size*64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convo_2_flat_ext = tf.concat( [convo_2_flat, x_feat ], 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_feature = tf.concat( [full_one_dropout, x_feat], 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = normal_full_layer(full_one_dropout,len(part_list))\n",
    "y_pred = normal_full_layer(full_feature, len(part_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Graph session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on step 0\n",
      "Accuracy is:\n",
      "0.0833333\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-567-75bfdd43ed23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1571\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1572\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m           self._build_eager(\n",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    index = 0\n",
    "    saver = tf.train.Saver()\n",
    "    save_path='./output/teeth_learn'\n",
    "\n",
    "    for i in range(5000):\n",
    "        batch_size = 2\n",
    "        x_dataset = train_dataset[index:index+batch_size].reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "        y_dataset = label_dataset[index:index+batch_size].reshape(-1, len(part_list))\n",
    "        f_dataset = feature_dataset[index:index+batch_size].reshape(-1, FEATURE_SIZE)\n",
    "\n",
    "        \n",
    "        index = (index+batch_size) % len(train_dataset)\n",
    "        \n",
    "        sess.run(train, feed_dict={x: x_dataset, y_true: y_dataset, x_feat: f_dataset, hold_prob: 0.5})\n",
    "#        sess.run(train, feed_dict={x: x_dataset, y_true: y_dataset, hold_prob: 0.5})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print('Currently on step {}'.format(i))\n",
    "            print('Accuracy is:')\n",
    "            # Test the Train Model\n",
    "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "\n",
    "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "#            print(sess.run(acc,feed_dict={x:test_dataset, y_true:test_label, hold_prob:1.0}))\n",
    "            print(sess.run(acc,feed_dict={x:test_dataset, y_true:test_label, x_feat:test_feature, hold_prob:1.0}))\n",
    "            print('\\n')\n",
    "            \n",
    "            saver.save(sess, save_path, write_meta_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
